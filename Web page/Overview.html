<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>
<h1 style="text-align:center; font-size:250%;">Different Modules Used For Prediction</h1>
<ol>
    <li><p style="font-size:160%;">Logistic Regression:-<br>
        <br>
        Logistic Regression is a classification algorithm used in Machine Learning to predict the probability of a categorical dependent variable. The dependent variable in logistic regression is a binary variable that contains data coded as 1 (yes, success, etc.) or 0. (no, failure, etc.). To look at it another way, the logistic regression model forecasts P(Y=1) as a function of X.
    </p></li>
    <li><p style="font-size:160%;">Linear Regression:-<br>
        <br>
        Linear regression is probably one of the most important and widely used regression techniques. It’s among the simplest regression methods. One of its main advantages is the ease of interpreting results. Linear regression is one of the fundamental statistical and machine learning techniques.
    </p></li>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png" alt="Notebook" style="width:50%;">
    <li><p style="font-size:160%;">Random Forest :-<br>
        <br>
        Random forest is a versatile algorithm capable of performing both Regression and Classification. Random Forest will combine the results from all the decision trees and then it will finally come up with the result. It is commonly used for predictive modelling and machine learning technique.
        </p></li>
        <img src="https://static.javatpoint.com/tutorial/machine-learning/images/random-forest-algorithm.png" alt="Notebook" style="width:50%;">
    <li><p style="font-size:160%;">K-Nearest Neighbour:-<br>
        <br>
        The k-nearest neighbour's algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.
        KNN works by calculating the distances between a query and all of the examples in the data, then selecting the number of examples (K) closest to the query and voting for the most frequent label (in the case of classification) or averaging the labels (in the case of regression).
        </p></li>
        <img src="https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png" alt="Notebook" style="width:50%;">
    <li><p style="font-size:160%;">Long Short-Term Memory :-<br>
        <br>
        It is a special kind of neural network that is capable of learning long term dependencies in data. This is achieved because the recurring module of the model has a combination of four layers interacting with each other.An LSTM has a cell state and three gates which provides them with the power to selectively learn,unlearn or retain information from each of the units.
        </p></li>
        <img src="https://www.researchgate.net/publication/341126962/figure/fig2/AS:887452692647936@1588596618057/Flow-chart-of-the-LSTM-RNN-classifier-with-GA.ppm" alt="Notebook" style="width:50%;">
    <li><p style="font-size:160%;">Lasso Regression :-<br>
        <br>
        Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.
<br>
Lasso Regression uses L1 regularization technique (will be discussed later in this article). It is used when we have more features because it automatically performs feature selection.
    </p></li>
    <li><p style="font-size:160%;">Ridge Regression:-<br>
        <br>
        Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values. 
        <br>
        The cost function for ridge regression:
<br>
<br>Min(||Y – X(theta)||^2 + λ||theta||^2)<br>
<br>
Lambda is the penalty term. λ given here is denoted by an alpha parameter in the ridge function. So, by changing the values of alpha, we are controlling the penalty term. The higher the values of alpha, the bigger is the penalty and therefore the magnitude of coefficients is reduced.
    </p></li>
    <img src="https://www.andreaperlato.com/img/ridge.png" alt="Notebook" style="width:50%;">
  </ol>
</body>
</html>